swagger: '2.0'
info:
  title: OpenAI-Global
  description: >-
    This is a GLOBAL Connector meaning you embed the API Key into each of the
    definitions as a hidden attribute with a default value. To connect to the
    OpenAI API and use the Power of GPT3, API key must be entered as "Bearer
    YOUR_API_KEY"
  version: '2.1'
  contact:
    name: David Soden & Robin RosengrÃ¼n
    url: https://DavidSoden.com
host: api.openai.com
basePath: /
schemes:
  - https
consumes: []
produces: []
paths:
  /v1/engines/{engine}/completions:
    post:
      responses:
        default:
          description: default
          schema: {}
      summary: GPT3 Completes your prompt
      description: GPT3 Completes your prompt
      operationId: Completion
      x-ms-visibility: important
      parameters:
        - name: engine
          in: path
          required: true
          type: string
          default: text-davinci-003
          description: >-
            The used engine, choose between text-davinci-002/003,
            text-curie-001, text-babbage-001, text-ada-001
          x-ms-summary: Engine
          x-ms-visibility: important
          enum:
            - text-davinci-003
            - text-davinci-002
            - text-curie-001
            - text-babbage-001
            - text-ada-001
        - name: access-token
          in: header
          required: true
          type: string
          default: Bearer YOUR_API_KEY_GOES_HERE_AND_IS_HIDDEN
          description: >-
            Connect to the OpenAI API and use the Power of GPT3, API key must be
            entered as "Bearer YOUR_API_KEY"
          x-ms-summary: Bearer YOUR_API_KEY
          x-ms-visibility: internal
        - name: body
          in: body
          required: false
          schema:
            type: object
            properties:
              prompt:
                type: string
                description: Text that will be completed by GPT3
                title: prompt
                default: >-
                  What is your favorite animal and why? Tell me also about the
                  size and weight of this animal.
              'n':
                type: integer
                format: int32
                description: How many completions to generate for each prompt
                title: ''
                default: 1
              best_of:
                type: integer
                format: int32
                description: >-
                  If set to more than 1, generates multiple completions
                  server-side and returns the "best". Must be greater than "n".
                  Use with caution, can consume a lot of tokens.
                title: ''
                default: 1
              temperature:
                type: integer
                format: int32
                description: >-
                  Higher values means the model will take more risks. Try 0.9
                  for more creative applications, and 0 (argmax sampling) for
                  ones with a well-defined answer. Use this OR top p
                title: temperature
                default: 1
              max_tokens:
                type: integer
                format: int32
                description: >-
                  One token equals roughly 4 characters of text (up to 4000
                  tokens between prompt and completion)
                title: max tokens
                default: 100
              top_p:
                type: integer
                format: int32
                description: >-
                  An alternative to sampling with temperature, called nucleus
                  sampling, where the model considers the results of the tokens
                  with top_p probability mass. So 0.1 means only the tokens
                  comprising the top 10% probability mass are considered.
                title: top_p
              frequency_penalty:
                type: integer
                format: int32
                description: >-
                  Number between -2.0 and 2.0. Positive values penalize new
                  tokens based on their existing frequency in the text so far,
                  decreasing the models likelihood to repeat the same line
                  verbatim.
                title: frequency penalty
                default: 0
              presence_penalty:
                type: integer
                format: int32
                description: >-
                  Number between -2.0 and 2.0. Positive values penalize new
                  tokens based on whether they appear in the text so far,
                  increasing the models likelihood to talk about new topics.
                title: presence_penalty
                default: 0
              user:
                type: string
                description: >-
                  A unique identifier representing your end-user, which will
                  help OpenAI to monitor and detect abuse
                title: user
              stop:
                type: array
                items:
                  type: string
                description: stop
            required:
              - prompt
definitions: {}
parameters: {}
responses: {}
securityDefinitions: {}
security: []
tags: []
